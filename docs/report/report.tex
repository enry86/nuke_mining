\documentclass{acm_proc_article-sp-sigmod07}

\begin{document}

\title{Framework for comparison between classification algorithms}
\numberofauthors{2}
\author{Luca Gasparella, Enrico Sartori}

\maketitle

\begin{abstract}
This paper presents a framework useful to perform comparisons between
classification algorithms.
The main evaluation measures used are the time used for building the
classification structure, the space consumed by the structure and the
accuracy of the classification provided.

The example provided here is the evaluation of the performances of a
generic decision tree and the Random Decision Tree algorithm by Wei Fan.
\end{abstract}

\section{INTRODUCTION}
The main objective of this work is to measure and compare the performances
of different classification algorithms. In particular we have focused on
the Random Decision Tree algorithm, comparing its performances with a
decision tree based on Information Gain.

During the development of the project we took care of two different
aspects, in fact we provided an implementation of the algorithms tested
and we have implemented a framework able to launch and measure the
performance of the different algorithms.

The evaluation of the algorithm is based on three measures: the time spent
for build the classification structure, the space needed to store and use
this structure and the accuracy of the algorithm.

The accuracy is defined as the percentage of correctly classified
datapoints in a dataset.
For allowing the analysis of the accuracy on every dataset, our framework
integrates the possibility to perform a cross validation over the dataset
using a user chosen percentage of datapoints for training ant the other
for testing.

The testing environment realized permits the setting of many parameters
used for tuning the classification task, in order to measure the influence
of those parameters on the overall performance.

The language chosen for the implementation of the platform and the
algorithms is Python. This choice has been made because of the relative
simplicity of the language which permitted us to focus more attention on
the algorithmic part rather than on solving more fine grain implementation
problems.

In the test section of this work we analyze how the different algorithms
behave when used on different dataset and with different parameters. 

\section{RELATED WORK}

The first algorithm used for comparison is a common Decision Tree, in fact
we wanted to have a comparison of the RDT approach with a commonly used
and known method.
The literature is rich of works about decision tree training and
implementation. In particular \cite{quinlan:induction} gives a complete
background of the basilar techniques used for the realization of such
algorithms.

In this work particular attention is posed into the various measures used
to evaluate the best attribute to use to train the root of a decision
tree.
These techniques are presented in a more concrete way in a tutorial by
Andrew Moore \cite{moore:dtree}. In this presentation the algorithm used is
a decision tree based on information gain as splitting measure, and
presents some ways to optimize and prune the obtained tree.
For the information gain, the same author published a set of slides
in which presents this measure and the way in which it can be computed.

The second algorithm we choose for the comparison is the Random Decision
Tree presented by Wei Fan in \cite{fan:rdt}

\section{PROBLEM DEFINITION}
The main aim of this work is to produce a comparison between two different
approaches to the classification task. In particular we are focusing our
attention in analyzing the performances of a rather new approach (RDT),
which uses random choices ti build the decision tree, and a more common
algorithm based on information gain.

In order to to this we need to provide a reliable implementation of both
the algorithms, which permits us to perform some test and analysis on the
results acquired.

The measures used for comparison are three: 
\begin{itemize}
\item The time taken to build the tree and eventually perform the
classification three.
\item The space used to store the tree structure.
\item The accuracy obtained in the classification task.
\end{itemize}

The measure of the space used presented some difficulties, due to the
dynamic typing feature of the language used, in order to solve this we
implemented a method which measures the memory used by an object referring
to the correspondent primitive type of C. This method is explained in
details in following section.

The accuracy can be verified in case of cross validation, classifying a
set of previously known datapoints. This measure can vary depending on the
dataset characteristics. For this reason is important to compare the
results obtained by the two algorithms on the same datasets.

\section{PROPOSED APPROACH}
This section describes and discusses the choice taken during the
development of the project. It will focus first on the implementation of
the platform used for the algorithm comparison and the way it's intended
to be used.
Later the details about the implementation of the algorithms will be
clarified.

\subsection{Classification platform}
This part of the project covers the task of managing the execution of the
algorithms and the measurement of the performances. As for the
classification algorithms, the language chosen for the implementation is
python, in order to have a more natural interface with the classifier.

The main tasks which this section of the software performs, can be
summarized as follows:
\begin{itemize}
\item Interpret the command line options;
\item Execute the selected algorithm following the options given;
\item Provide support for cross validation;
\item Measure the performance obtained by the algorithm executed;
\item Report the informations retrieved;
\end{itemize}

Before discuss the implementation details is useful give a quick reference
about the usage of the system.

\subsubsection{System usage}
The platform implemented permits the setting of various options, needed
for widen the possibility of performing tests with different algorithms.

The general way in which the system operates is to use a dataset for train
the classifier. Then the platform reads from test dataset and classifies
the datapoints, finally the now classified data are stored in a third
dataset.

The user can specify the attribute to be used as class and a list of
attributes to be ignored in the training task.

One of the most relevant features is the support for cross validation,
with the possibility to select the percentage of data to use for training
and for testing.
When cross validation is enabled is possible to compute a measure of
accuracy of the algorithm. 

Obviously is possible to select the desired algorithm to be used for the
test. In addition to this is possible to choose a particular aspect of the
algorithm itself.

In fact, for the generic Decision Tree is possible to choose the number of
threshold to be tested for the computation of the information gain for
numerical data.
A similar option is usable for the RDT, permitting to set the number of
random tree to be generated.

An outline of the possible options is provided by the following list:
\begin{description}
\item{-t} File containing the training dataset, mandatory
\item{-d} File containing the testing dataset, mandatory if not in cross
validation mode
\item{-o} Output file, mandatory
\item{-a} Name of the class attribute, mandatory
\item{-c} Classifier (classifier\_name[,extra\_parameter]), optional
(generic DT is the default choice), possible names are generalClass or
randomDT
\item{-i} List of ignored attributes, optional
\item{-x} Percentage of splitting for cross validation, optional, enables
cross validation
\end{description}

Following the previous outline, an example of usage of the system could be
this example:
\begin{verbatim}
./nukeminer -t training.arff -o output.arff \
    -c randomDT,25 -a class -x 60
\end{verbatim}

Launched with this parameters the system executes a cross validation on
the dataset training.arff, classifying on the attribute class. The system
will use the 60\% of the data for training and the 40\% for testing. The
output will be stored in the file output.arff, the algorithm used is the
RDT and will use 25 random trees.

At the end of the execution the system will output the time taken for
train and test the classifier, the memory used to store the data structure
used, and the accuracy reached in this execution.

It's important to notice that the split of the dataset for the cross
validation is performed following a random approach. In this way the
original distribution of the data is preserved in both training and
testing datasets.

This is important to perform a good training of the classifier,
particularly in datasets characterized by having most of the data all in a
single class. This could lead to the situation in which there are no data
belonging to minority class in the training dataset.
Obviously this is a bad situation for training the algorithm.

The technique used by this platform is to scan the dataset and decide for
each datapoint if it belongs to the training or to the testing dataset,
the probability of the outcome of this decision is the one specified by
the user via command line.
This technique assures a more fair division of the dataset.

\subsubsection{File format}
An important remark has to be done for the format of input/output files,
in fact the platform uses a particular file format for handling the data.
It's a simpler version of the ARFF syntax, adapted to fit the needs of the
classification task to be performed.

Is possible to define two types of data: numeric or string. The numeric
type covers both integer and float values, while string is used for
handling categorical data.

For example the following lines represent the header of a dataset file:

\begin{verbatim}
@RELATION iris

@ATTRIBUTE sepallength	numeric
@ATTRIBUTE sepalwidth 	numeric
@ATTRIBUTE petallength 	numeric
@ATTRIBUTE petalwidth	numeric
@ATTRIBUTE class string 

@DATA
\end{verbatim}

This is the adapted header of the Iris dataset, after the keyword DATA
is possible to write the tuples, as a list of values separated by a ','
character, as in the following example:

\begin{verbatim}
@DATA
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
\end{verbatim}

\subsubsection{Performance evaluation}
The main objective of the platform is to provide a system able to measure
the performance of a particular algorithm completing the task assigned.
As said in the introduction the system bases the evaluation on three main
measures: time, space and accuracy of the classification.

The classification process goes through two main phases, the first one,
the most long and difficult, is the construction of the decision tree,
while the classification of unknown datapoints is generally a less
demanding operation.

The platform presented here can evaluate the time needed both for
constructing the tree and for classifying a test dataset. 
Even if the time needed for classifying a dataset is often a fraction of
the time used for building the tree, the behaviour of different algorithms
on this operation can be very important.

An example of the importance of this value can be represented by the case
of an on-line classifier. In this particular environment the time needed
for classify a datapoint is a vital parameter for the evaluation of the
performance of the algorithm.

The technique used for evaluate the time spent on the whole classification
task is fairly simple. In fact the platform simply measures the time
elapsed between the start and the end of the process.

Besides its simplicity and robustness, this approach presents some
drawbacks. The main is that this way of measuring time performance is
strictly dependent on the machine used for executing the test.
Therefore in order to retrieve reliable result from a comparison between
two algorithms is better to execute them on the same machine.

Another problem is represented by the possibility of interferences from
other programs and daemon running on the machine. These interferences can
lead to a drop of performances in term of time.

An alternative way of counting the time spent for the execution of the
algorithm could be use, instead of the second elapsed, the clock cycles
used by the program. 
This measure unit would be less immediate to understand and less natural.

During the development of the project has been preferred to use seconds as
the measure unit, taking care of execute the program in controlled
environment on a dedicated machine.

The measure of the space is equally important, in fact the dimension of
the dataset to analyze can be very huge, or theoretically infinite, as in
the case of many application of online classifiers.
This shows the importance of the measure of the space used for running the
classification algorithm, the risk is to build a structure which cannot
fit in the main memory of the machine used.

For the algorithms implemented for this comparison, this case has not been
considered, considering valid the assumption that the dataset can always
fit in main memory.
The reason of this choice is that implement some ways to reduce the
footprint in memory of the structure used, maybe storing some less used
branches of the tree on disk, could be very difficult to be realized in
equal way between the two algorithms.

The differences between the structures used in the two algorithms, are too
relevant to assume that the same time will be spent in storing and reading
the cached data from disk. These difference will corrupt the results in
time evaluation.

Moreover, avoiding the usage of caching techniques, is possible to
retrieve a detailed picture of the actual memory usage of the algorithm at
any time. 

Another remark about measuring the memory used by the structure regards
the language used for the implementation.
In fact python features dynamic typing, this poses difficulties in the
evaluation of the actual amount of memory used to store a
particular object.

For dealing with this issue,the memory used by an object is computed using
the size of the C primitives types corresponding to the python types used.

The last measure considered for evaluate the performances of the
algorithm is the quality, or accuracy, of the classification performed. 
This measure is very indicative in particular if compared with the time
measure.
In fact often, in many problems of computer science, the solution of a
problem is a balance of the trade-off between time and quality.

In the platform presented here the computation of the quality of the
classification obtained is possible only when the system runs in cross
validation mode. In fact, when performing this kind of test, is sure that
even the testing dataset contains the correct classification.

Therefore is easy to compare the classification computed by the algorithm
with the real values in the dataset.
The system finally computes the percentage of correctly classified
datapoints over the size of the testing dataset.
This value is considered the measure of the quality if the classification
obtained.

A possible drawback of this approach is represented by the particular
situation in which a very large majority of the training datapoints
belongs to the same class.
The building of the decision tree, therefore, will be based on a large
amount of informations from the largest class, while the other will be
less represented in the final tree.

A direct consequence of this is that the classifier will chose as class
the major one more likely, generating many misclassifications for the
least relevant class.

The level of accuracy, anyway, will remain quite high, this because the
large even in the test dataset the large majority of the data will belong
to the most probable class. The elements in the least relevant class are
so few that, even if most of them are not correctly classified, the
accuracy level doesn't reflect this situation.

\subsubsection{Reporting}
The last task for the platform is to output some form of report
illustrating the result obtained from an experiment.
The method chosen for this platform is to implement a bash script that
manages the execution of the two algorithms several times with different
parameter on a dataset.

This script reads the output of the program and collect the useful data,
writing the results in a format usable with the utility gnuplot for being
transfered into charts.

This method provides a simple way to have an intuitive comparison between the
results of the two different algorithms.

\subsection{Decision Tree}
The first algorithm considered for the comparison is a generic Decision
Tree. Has been chosen because it's a well known approach and is a good
base for a comparison with another algorithm.

The measure chosen for selecting the attribute on which split the dataset

\bibliographystyle{abbrv}
\bibliography{report}
\end{document}
