\documentclass{acm_proc_article-sp-sigmod07}

\begin{document}

\title{Framework for comparison between classification algorithms}
\numberofauthors{2}
\author{Luca Gasparella, Enrico Sartori}

\maketitle

\begin{abstract}
This paper presents a framework useful for the comparison between
classification algorithms.
The main evaluation measures used are the time used for building the
classification structure, the space consumed by the structure and the
accuracy of the classification provided.

The example provided here is the evaluation of the performances of a
generic decision tree and the Random Decision Tree algorithm by Wei Fan.
\end{abstract}

\section{INTRODUCTION}
The main objective of this work is to measure and compare the performances
of different classification algorithms. In particular we have focused on
the Random Decision Tree algorithm, comparing its performances with a
decision tree based on Information Gain.

During the development of the project we took care of two different
aspects, in fact we provided an implementation of the algorithms tested
and we have implemented a framework able to launch and measure the
performance of the different algorithms.

The evaluation of the algorithm is based on three measures: the time spent
for build the classification structure, the space needed to store and use
this structure and the accuracy of the algorithm.

The accuracy is defined as the percentage of correctly classified
datapoints in a dataset.
For allowing the analysis of the accuracy on every dataset, our framework
integrates the possibility to perform a cross validation over the dataset
using a user chosen percentage of datapoints for training ant the other
for testing.

The testing environment realized permits the setting of many parameters
used for tuning the classification task, in order to measure the influence
of those parameters on the overall performance.

The language chosen for the implementation of the platform and the
algorithms is Python. This choice has been made because of the relative
simplicity of the language which permitted us to focus more attention on
the algorithmic part rather than on solving more fine grain implementation
problems.

In the test section of this work we analyze how the different algorithms
behave when used on different dataset and with different parameters. 

\section{RELATED WORK}

The first algorithm used for comparison is a common Decision Tree, in fact
we wanted to have a comparison of the RDT approach with a commonly used
and known method.
The literature is rich of works about decision tree training and
implementation. In particular \cite{quinlan:induction} gives a complete
background of the basilar techniques used for the realization of such
algorithms.

In this work particular attention is posed into the various measures used
to evaluate the best attribute to use to train the root of a decision
tree.
These techniques are presented in a more concrete way in a tutorial by
Andrew Moore \cite{moore:dtree}. In this presentation the algorithm used is
a decision tree based on information gain as splitting measure, and
presents some ways to optimize and prune the obtained tree.
For the information gain, the same author published a set of slides
in which presents this measure and the way in which it can be computed.

The second algorithm we choose for the comparison is the Random Decision
Tree presented by Wei Fan in \cite{fan:rdt}

\section{PROBLEM DEFINITION}
The main aim of this work is to produce a comparison between two different
approaches to the classification task. In particular we are focusing our
attention in analyzing the performances of a rather new approach (RDT),
which uses random choices ti build the decision tree, and a more common
algorithm based on information gain.

In order to to this we need to provide a reliable implementation of both
the algorithms, which permits us to perform some test and analysis on the
results acquired.

The measures used for comparison are three: 
\begin{itemize}
\item The time taken to build the tree and eventually perform the
classification three.
\item The space used to store the tree structure.
\item The accuracy obtained in the classification task.
\end{itemize}

The measure of the space used presented some difficulties, due to the
dynamic typing feature of the language used, in order to solve this we
implemented a method which measures the memory used by an object referring
to the correspondent primitive type of C. This method is explained in
details in following section.

The accuracy can be verified in case of cross validation, classifying a
set of previously known datapoints. This measure can vary depending on the
dataset characteristics. For this reason is important to compare the
results obtained by the two algorithms on the same datasets.

\section{PROPOSED APPROACH}
This section describes and discusses the choice taken during the
development of the project. It will focus first on the implementation of
the platform used for the algorithm comparison and the way it's intended
to be used.
Later the details about the implementation of the algorithms will be
clarified.

\subsection{Classification platform}
This part of the project covers the task of managing the execution of the
algorithms and the measurement of the performances. As for the
classification algorithms, the language chosen for the implementation is
python, in order to have a more natural interface with the classifier.

The main tasks which this section of the software performs, can be
summarized as follows:
\begin{itemize}
\item Interpret the command line options;
\item Execute the selected algorithm following the options given;
\item Provide support for cross validation;
\item Measure the performance obtained by the algorithm executed;
\item Report the informations retrieved;
\end{itemize}

Before discuss the implementation details is useful give a quick reference
about the usage of the system.

\subsubsection{System usage}
The platform implemented permits the setting of various options, needed
for widen the possibility of performing tests with different algorithms.

The general way in which the system operates is to use a dataset for train
the classifier. Then the platform reads from test dataset and classifies
the datapoints, finally the now classified data are stored in a third
dataset.

The user can specify the attribute to be used as class and a list of
attributes to be ignored in the training task.

One of the most relevant features is the support for cross validation,
with the possibility to select the percentage of data to use for training
and for testing.
When cross validation is enabled is possible to compute a measure of
accuracy of the algorithm. 

Obviously is possible to select the desired algorithm to be used for the
test. In addition to this is possible to choose a particular aspect of the
algorithm itself.

In fact, for the generic Decision Tree is possible to choose the number of
threshold to be tested for the computation of the information gain for
numerical data.
A similar option is usable for the RDT, permitting to set the number of
random tree to be generated.

An outline of the possible options is provided by the following list:
\begin{description}
\item{-t} File containing the training dataset, mandatory
\item{-d} File containing the testing dataset, mandatory if not in cross
validation mode
\item{-o} Output file, mandatory
\item{-a} Name of the class attribute, mandatory
\item{-c} Classifier (classifier\_name[,extra\_parameter]), optional
(generic DT is the default choice), possible names are generalClass or
randomDT
\item{-i} List of ignored attributes, optional
\item{-x} Percentage of splitting for cross validation, optional, enables
cross validation
\end{description}

Following the previous outline, an example of usage of the system could be
this example:
\begin{verbatim}
./nukeminer -t training.arff -o output.arff \
    -c randomDT,25 -a class -x 60
\end{verbatim}

Launched with this parameters the system executes a cross validation on
the dataset training.arff, classifying on the attribute class. The system
will use the 60\% of the data for training and the 40\% for testing. The
output will be stored in the file output.arff, the algorithm used is the
RDT and will use 25 random trees.

At the end of the execution the system will output the time taken for
train and test the classifier, the memory used to store the data structure
used, and the accuracy reached in this execution.

It's important to notice that the split of the dataset for the cross
validation is performed following a random approach. In this way the
original distribution of the data is preserved in both training and
testing datasets.

This is important to perform a good training of the classifier,
particularly in datasets characterized by having most of the data all in a
single class. This could lead to the situation in which there are no data
belonging to minority class in the training dataset.
Obviously this is a bad situation for training the algorithm.

The technique used by this platform is to scan the dataset and decide for
each datapoint if it belongs to the training or to the testing dataset,
the probability of the outcome of this decision is the one specified by
the user via command line.
This technique assures a more fair division of the dataset.

\subsubsection{File format}
An important remark has to be done for the format of input/output files,
in fact the platform uses a particular file format for handling the data.
It's a simpler version of the ARFF syntax, adapted to fit the needs of the
classification task to be performed.

Is possible to define two types of data: numeric or string. The numeric
type covers both integer and float values, while string is used for
handling categorical data.

For example the following lines represent the header of a dataset file:

\begin{verbatim}
@RELATION iris

@ATTRIBUTE sepallength	numeric
@ATTRIBUTE sepalwidth 	numeric
@ATTRIBUTE petallength 	numeric
@ATTRIBUTE petalwidth	numeric
@ATTRIBUTE class string 

@DATA
\end{verbatim}

This is the adapted header of the Iris dataset, after the keyword DATA
is possible to write the tuples, as a list of values separated by a ','
character, as in the following example:

\begin{verbatim}
@DATA
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
\end{verbatim}


\bibliographystyle{abbrv}
\bibliography{report}
\end{document}
